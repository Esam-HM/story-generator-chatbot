{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXecIELe45Sz"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "from datasets import Dataset,DatasetDict"
      ],
      "metadata": {
        "id": "tR2GCOSI5AaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "file_path = \"/content/drive/MyDrive/AI/final_dataset2.txt\"\n",
        "model_name = \"ytu-ce-cosmos/turkish-gpt2\""
      ],
      "metadata": {
        "id": "HGIzCIuf5FPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "## Define tokens used in dataset, and set them to the tokenizer used.\n",
        "special_tokens_dict = {\n",
        "    \"bos_token\": \"<BOS>\",\n",
        "    \"eos_token\": \"<EOS>\",\n",
        "    \"pad_token\": \"<PAD>\",\n",
        "    \"additional_special_tokens\": [\"<Title>\",\"<EndTitle>\"]\n",
        "    }\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
      ],
      "metadata": {
        "id": "8rFxh3ui5JRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This function used to read dataset.\n",
        "## Returns dataset contains tensors.\n",
        "def load_dataset(file_path, tokenizer, block_size=750):\n",
        "  stories = []\n",
        "  with open(file_path,\"r\") as file:\n",
        "    for line in file:\n",
        "      line = line.strip()           ## Cleans Text from whitspace characters.\n",
        "      if line and len(tokenizer(line)['input_ids'])<=block_size:\n",
        "        stories.append(line)\n",
        "  print(f\"Number of Stories: {len(stories)}\")\n",
        "  ##return LineByLineTextDataset(tokenizer,file_path,block_size)\n",
        "  return Dataset.from_dict(tokenizer(stories, truncation=True, padding=True, max_length=block_size, return_tensors=\"pt\"))\n",
        "\n",
        "## This function used to load data to model.\n",
        "def load_data_collator(tokenizer):\n",
        "    return DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "## This function defines model parameters.\n",
        "def train(train_file_path, model_name,output_dir,overwrite_output_dir,per_device_train_batch_size,num_train_epochs,save_steps):\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "  model.save_pretrained(output_dir)\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=output_dir,\n",
        "      overwrite_output_dir=overwrite_output_dir,\n",
        "      per_device_train_batch_size=per_device_train_batch_size,\n",
        "      num_train_epochs=num_train_epochs,\n",
        "      save_steps=save_steps,\n",
        "      save_strategy='no'\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      data_collator=data_collator,\n",
        "      train_dataset=train_dataset\n",
        "      )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "2o_lkyY25NFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define parameters and call train function to start training.\n",
        "output_dir = '/content/drive/MyDrive/AI/smallGPT2'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 2\n",
        "num_train_epochs = 10.0\n",
        "save_steps = 500\n",
        "\n",
        "train(\n",
        "    train_file_path=file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "id": "MT5KkJjL9wMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This function used to load model stored in output_dir\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    model_path = '/content/drive/MyDrive/AI/smallGPT2'\n",
        "    model = load_model(model_path)\n",
        "    model = model.to(torch.device('cuda'))\n",
        "\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt').to(torch.device('cuda'))\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        max_length=max_length,\n",
        "        eos_token_id=tokenizer.eos_token_id,        ## Define tokens used.\n",
        "        repetition_penalty=1.2,                     ## Apply repetition penalty\n",
        "        early_stopping= True ,\n",
        "        num_beams = 3,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0]))"
      ],
      "metadata": {
        "id": "_sFF9M-y90HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = \"<BOS> <Title> Ağlayan Çocuk Masalı <EndTitle>\"\n",
        "generate_text(seq,750)"
      ],
      "metadata": {
        "id": "hTCqdc2SRx4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = \"<BOS> <Title> Yalancı Mahmut Masalı <EndTitle>\"\n",
        "generate_text(seq,750)"
      ],
      "metadata": {
        "id": "c2EA0Pxo_zIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = \"<BOS> <Title> Ali'nin Doğum Günü Hikayesi <EndTitle>\"\n",
        "generate_text(seq,750)"
      ],
      "metadata": {
        "id": "UyWALE6sknSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = \"<BOS> <Title> Selma'nın Zor Ödevi Hikayesi <EndTitle>\"\n",
        "generate_text(seq,750)"
      ],
      "metadata": {
        "id": "qJyrBdfcDa6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}